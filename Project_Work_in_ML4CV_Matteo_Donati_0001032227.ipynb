{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BFVvggVfZuN1",
        "rk5IqtHwZyfV",
        "axkF_Bd1F_eM",
        "34JWWDLFPwJi",
        "ClULEfjlPu56",
        "PHqUD8h6Gzb8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <h1> Machine Learning for Computer Vision</h1>\n",
        "    <h2> Project Work </h2>\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Student**: Matteo Donati <br>\n",
        "**Student ID**: 0001032227 <br>\n",
        "**E-mail**: <a href=\"mailto:matteo.donati10@studio.unibo.it\">matteo.donati10@studio.unibo.it</a>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "hgXhyNHqFtSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an unofficial implementation of the paper by Guo et al., (2021)<sup>[[1]](#references)</sup>."
      ],
      "metadata": {
        "id": "mXVx3Q1PGpAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "BFVvggVfZuN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T\n",
        "import multiprocessing as mp\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "SA3HZYkVZrxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_reproducibility(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "set_reproducibility(0)"
      ],
      "metadata": {
        "id": "Gx8K3KIcgJMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "rk5IqtHwZyfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieves the CIFAR10 dataset.\n",
        "def get_cifar10(train_transform = None, test_transform = None):\n",
        "\n",
        "  # Downloading the training set.\n",
        "  train_dataset =  datasets.CIFAR10(\n",
        "    root = \"CIFAR10\",\n",
        "    train = True,\n",
        "    transform = train_transform,\n",
        "    download = True\n",
        "  )\n",
        "\n",
        "  # Downloading the test set.\n",
        "  test_dataset = datasets.CIFAR10(\n",
        "    root = \"CIFAR10\",\n",
        "    train = False,\n",
        "    transform = test_transform,\n",
        "    download = True\n",
        "  )\n",
        "\n",
        "  # Returning the two sets.\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "# Fuction that creates train, validation and test data loaders.\n",
        "def create_data_loaders(train_transform, \n",
        "                        test_transform, \n",
        "                        img_size = 224, \n",
        "                        split = (0.8, 0.2), \n",
        "                        batch_size = 32, \n",
        "                        num_workers = 1):\n",
        "\n",
        "  # Retrieving CIFAR10.\n",
        "  train_dataset, test_dataset = get_cifar10(train_transform, test_transform)\n",
        "\n",
        "  # Splitting train_dataset to create a validation set.\n",
        "  train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, \n",
        "                                                             (int(len(train_dataset) * split[0]), \n",
        "                                                              int(len(train_dataset) * split[1])))\n",
        "\n",
        "  # Train data loader.\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    drop_last = True,\n",
        "    sampler = None\n",
        "  )\n",
        "\n",
        "  # Validation data loader.\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    drop_last = False,\n",
        "    sampler = None\n",
        "  )\n",
        "\n",
        "  # Test data loader.\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    drop_last = False,\n",
        "    sampler = None\n",
        "  )\n",
        "\n",
        "  # Returning the three data loaders.\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "sTkFIuugZxtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training transformations.\n",
        "train_transform = T.Compose([\n",
        "    T.RandomCrop(32, padding = 4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(10),\n",
        "    T.Resize(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean = [0.491, 0.482, 0.447], std = [0.247, 0.243, 0.262])\n",
        "])\n",
        "\n",
        "# Test transformations.\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean = [0.491, 0.482, 0.447], std = [0.247, 0.243, 0.262])\n",
        "])\n",
        "\n",
        "# Creating data loaders.\n",
        "train_loader, val_loader, test_loader = create_data_loaders(\n",
        "    train_transform,\n",
        "    test_transform,\n",
        "    img_size = 224,\n",
        "    batch_size = 64,\n",
        "    num_workers = mp.cpu_count()\n",
        ")"
      ],
      "metadata": {
        "id": "2-yP8OHGhPuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Definition and Training"
      ],
      "metadata": {
        "id": "bpKz7V32su2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities"
      ],
      "metadata": {
        "id": "axkF_Bd1F_eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing between GPU and CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "oaT-QCAapgdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stem module of the CMT architecture.\n",
        "class Stem(nn.Module):\n",
        "    \n",
        "  # Constructor. Requires number of input and output channels.\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Conv 3 x 3, stride 2.\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # Conv 3 x 3. \n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # Conv 3 x 3.\n",
        "    self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # GELU activation.\n",
        "    self.gelu = nn.GELU()\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.gelu(x)\n",
        "    y = self.bn3(x)\n",
        "    return y"
      ],
      "metadata": {
        "id": "bAsbc7UBsoy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch embedding module of the CMT architecture.\n",
        "class PatchEmbedding(nn.Module):\n",
        "\n",
        "  # Constructor. Requires number of input and output channels.\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Conv 2 x 2, stride 2.\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    _, c, h, w = x.size()\n",
        "    y = torch.nn.functional.layer_norm(x, (c, h, w))\n",
        "    return y"
      ],
      "metadata": {
        "id": "7BVXE1Jw0HoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Perception Unit: LPU(X) = DWConv(X) + X.\n",
        "class LPU(nn.Module):\n",
        "\n",
        "  # Constructor.\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Depthwise convolution.\n",
        "    self.dwconv = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, groups = in_channels)\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    y = self.dwconv(x) + x\n",
        "    return y"
      ],
      "metadata": {
        "id": "OFDPglv5iNZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lightweight Multi-Head-Self-Attention.\n",
        "class LMHSA(nn.Module):\n",
        "    \n",
        "  # Constructor.\n",
        "  def __init__(self, input_size, channels, d_k, d_v, stride, heads):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Depthwise convolutions.\n",
        "    self.dwconv = nn.Conv2d(channels, channels, kernel_size = stride, stride = stride, groups = channels)\n",
        "\n",
        "    # Projection matrices.\n",
        "    self.fc_q = nn.Linear(channels, heads * d_k)\n",
        "    self.fc_k = nn.Linear(channels, heads * d_k)\n",
        "    self.fc_v = nn.Linear(channels, heads * d_v)\n",
        "    self.fc_o = nn.Linear(heads * d_k, channels)\n",
        "\n",
        "    self.channels = channels\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.stride = stride\n",
        "    self.heads = heads\n",
        "\n",
        "    # Relative position bias to each self-attention module.\n",
        "    self.B = nn.Parameter(torch.randn(1, self.heads, input_size ** 2, (input_size // stride) ** 2), requires_grad = True)\n",
        "    \n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Extracting shape from input signal.\n",
        "    b, c, h, w = x.shape\n",
        "\n",
        "    # Reshaping and permuting x. Final shape is (b, h * w, c).\n",
        "    x_reshape = x.view(b, c, h * w).permute(0, 2, 1)\n",
        "\n",
        "    # Layer norm.\n",
        "    x_reshape = torch.nn.functional.layer_norm(x_reshape, (b, h * w, c))\n",
        "\n",
        "    # Getting queries by applying the fc_q linear projection.\n",
        "    q = self.fc_q(x_reshape)\n",
        "\n",
        "    # Reshaping and permuting the queries. Final shape is (b, heads, n = h * w, d_k). \n",
        "    q = q.view(b, h * w, self.heads, self.d_k).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Applying depthwise conv to x.\n",
        "    k = self.dwconv(x)\n",
        "\n",
        "    # Extracting shape of keys.\n",
        "    k_b, k_c, k_h, k_w = k.shape\n",
        "\n",
        "    # Reshaping and permuting keys. Final shape is (k_b, k_h * k_w, k_c).\n",
        "    k = k.view(k_b, k_c, k_h * k_w).permute(0, 2, 1).contiguous()\n",
        "\n",
        "    # Projecting through fc_k.\n",
        "    k = self.fc_k(k)\n",
        "\n",
        "    # Reshaping and permuting the keys. Final shape is (k_b, heads, k_h * k_w, d_k).\n",
        "    k = k.view(k_b, k_h * k_w, self.heads, self.d_k).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Applying depthwise conv to x.\n",
        "    v = self.dwconv(x)\n",
        "\n",
        "    # Extracting shape of values.\n",
        "    v_b, v_c, v_h, v_w = v.shape\n",
        "\n",
        "    # Reshaping and permuting values. Final shape is (v_b, v_h * v_w, v_c).\n",
        "    v = v.view(v_b, v_c, v_h * v_w).permute(0, 2, 1).contiguous()\n",
        "\n",
        "    # Projecting through fc_v.\n",
        "    v = self.fc_v(v)\n",
        "\n",
        "    # Reshaping and permuting the keys. Final shape is (v_b, heads, v_h * v_w, d_v).\n",
        "    v = v.view(v_b, v_h * v_w, self.heads, self.d_v).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Computing softmax((Q K') / sqrt(d_k) + B). Final shape is (b, heads, n = h * w, k_h * k_w).\n",
        "    attention = torch.einsum(\"... i d, ... j d -> ... i j\", q, k) * (self.d_k ** -0.5)\n",
        "    attention = attention + self.B\n",
        "    attention = torch.softmax(attention, dim = -1)\n",
        "\n",
        "    # Applying attention scores to values by taking the dot product.\n",
        "    tmp = torch.matmul(attention, v).permute(0, 2, 1, 3)\n",
        "\n",
        "    # Permuting the result. Final shape is (b, n = h * w, heads, d_v).\n",
        "    tmp = tmp.contiguous().view(b, h * w, self.heads * self.d_v)\n",
        "\n",
        "    # Projecting using fc_o and reshaping. Final shape is (b, c, h, w).\n",
        "    tmp = self.fc_o(tmp).view(b, self.channels, h, w)\n",
        "\n",
        "    # Returning tmp + x (skip connection).\n",
        "    return tmp + x"
      ],
      "metadata": {
        "id": "mwVBiJHhlUY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Multi-Head-Self-Attention.\n",
        "class MHSA(nn.Module):\n",
        "    \n",
        "  # Constructor.\n",
        "  def __init__(self, input_size, channels, d_k, d_v, stride, heads):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Projection matrices.\n",
        "    self.fc_q = nn.Linear(channels, heads * d_k)\n",
        "    self.fc_k = nn.Linear(channels, heads * d_k)\n",
        "    self.fc_v = nn.Linear(channels, heads * d_v)\n",
        "    self.fc_o = nn.Linear(heads * d_k, channels)\n",
        "\n",
        "    self.channels = channels\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.stride = stride\n",
        "    self.heads = heads\n",
        "    \n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Extracting shape from input signal.\n",
        "    b, c, h, w = x.shape\n",
        "\n",
        "    # Reshaping and permuting x. Final shape is (b, h * w, c).\n",
        "    x_reshape = x.view(b, c, h * w).permute(0, 2, 1)\n",
        "\n",
        "    # Layer norm.\n",
        "    x_reshape = torch.nn.functional.layer_norm(x_reshape, (b, h * w, c))\n",
        "\n",
        "    # Getting queries by applying the fc_q linear projection.\n",
        "    q = self.fc_q(x_reshape)\n",
        "\n",
        "    # Reshaping and permuting the queries. Final shape is (b, heads, n = h * w, d_k). \n",
        "    q = q.view(b, h * w, self.heads, self.d_k).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Projecting through fc_k.\n",
        "    k = self.fc_k(x_reshape)\n",
        "\n",
        "    # Reshaping and permuting the keys. Final shape is (b, heads, h * w, d_k).\n",
        "    k = k.view(b, h * w, self.heads, self.d_k).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Projecting through fc_v.\n",
        "    v = self.fc_v(x_reshape)\n",
        "\n",
        "    # Reshaping and permuting the keys. Final shape is (b, heads, h * w, d_v).\n",
        "    v = v.view(b, h * w, self.heads, self.d_v).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Computing softmax((Q K) / sqrt(d_k)).\n",
        "    attention = torch.einsum(\"... i d, ... j d -> ... i j\", q, k) * (self.d_k ** -0.5)\n",
        "    attention = torch.softmax(attention, dim = -1)\n",
        "\n",
        "    # Applying attention scores to values by taking the dot product.\n",
        "    tmp = torch.matmul(attention, v).permute(0, 2, 1, 3)\n",
        "\n",
        "    # Permuting the result. Final shape is (b, n = h * w, heads, d_v).\n",
        "    tmp = tmp.contiguous().view(b, h * w, self.heads * self.d_v)\n",
        "\n",
        "    # Projecting using fc_o and reshaping. Final shape is (b, c, h, w).\n",
        "    tmp = self.fc_o(tmp).view(b, self.channels, h, w)\n",
        "\n",
        "    # Returning tmp + x (skip connection).\n",
        "    return tmp + x"
      ],
      "metadata": {
        "id": "Hdl8rdu8lb-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverted Residual Feed-forward Network: IRFFN(X) = Conv(F(Conv(X))), F(X) = DWConv(X) + X.\n",
        "class IRFFN(nn.Module):\n",
        "\n",
        "  # Constructor.\n",
        "  def __init__(self, in_channels, R):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Number of channels after expansion.\n",
        "    out_channels = int(in_channels * R)\n",
        "\n",
        "    # Conv 1 x 1.\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # Depthwise convolution.\n",
        "    self.dwconv = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, groups = out_channels)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # Conv 1 x 1.\n",
        "    self.conv2 = nn.Conv2d(out_channels, in_channels, kernel_size = 1)\n",
        "    self.bn3 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "    # GELU activation.\n",
        "    self.gelu = nn.GELU()\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    tmp = self.conv1(x)\n",
        "    tmp = self.bn1(tmp)\n",
        "    tmp = self.gelu(tmp)\n",
        "    tmp = self.dwconv(tmp)\n",
        "    tmp = self.bn2(tmp)\n",
        "    tmp = self.gelu(tmp)\n",
        "    tmp = self.conv2(tmp)\n",
        "    tmp = self.bn3(tmp)\n",
        "    y = x + tmp\n",
        "    return y"
      ],
      "metadata": {
        "id": "WQRfI_lQlYoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CMT block of the CMT architecture.\n",
        "class CMTBlock(nn.Module):\n",
        "\n",
        "  # Constructor. By default it loads the CMT-Ti configuration.\n",
        "  def __init__(self, img_size, k, d_k, d_v, num_heads, R = 3.6, in_channels = 46, attention_type = \"light\"):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Local Perception Unit.\n",
        "    self.lpu = LPU(in_channels, in_channels)\n",
        "\n",
        "    # Lightweight MHSA.\n",
        "    if attention_type == \"light\":\n",
        "      self.mhsa = LMHSA(img_size, in_channels, d_k, d_v, k, num_heads)\n",
        "    \n",
        "    # Lightweight MHSA.\n",
        "    elif attention_type == \"standard\":\n",
        "      self.mhsa = MHSA(img_size, in_channels, d_k, d_v, k, num_heads)\n",
        "    \n",
        "    # No attention.\n",
        "    else:\n",
        "      self.mhsa = None\n",
        "\n",
        "    # Inverted Residual FFN.\n",
        "    self.irffn = IRFFN(in_channels, R)\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    x = self.lpu(x)\n",
        "    if self.mhsa != None: x = self.mhsa(x)\n",
        "    y = self.irffn(x)\n",
        "    return y"
      ],
      "metadata": {
        "id": "sGf0VE9hhtVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CMT architecture.\n",
        "class CMT(nn.Module):\n",
        "\n",
        "  # Constructor. By default it loads the CMT-Ti configuration.\n",
        "  def __init__(self,\n",
        "    in_channels = 3,\n",
        "    stem_channels = 16,\n",
        "    cmt_channels = [46, 92, 184, 368],\n",
        "    patch_channels = [46, 92, 184, 368],\n",
        "    block_layers = [2, 2, 10, 2],\n",
        "    R = 3.6,\n",
        "    img_size = 224,\n",
        "    num_class = 10,\n",
        "    attention_type = \"light\"\n",
        "  ):\n",
        "\n",
        "    super(CMT, self).__init__()\n",
        "\n",
        "    # Stem layer\n",
        "    self.stem = Stem(in_channels, stem_channels)\n",
        "\n",
        "    # Patch Aggregation Layer\n",
        "    self.pe1 = PatchEmbedding(stem_channels, patch_channels[0])\n",
        "    self.pe2 = PatchEmbedding(patch_channels[0], patch_channels[1])\n",
        "    self.pe3 = PatchEmbedding(patch_channels[1], patch_channels[2])\n",
        "    self.pe4 = PatchEmbedding(patch_channels[2], patch_channels[3])\n",
        "\n",
        "    # Stage 1 CMT blocks.\n",
        "    stage1 = [CMTBlock(img_size = img_size // 4, \n",
        "                       k = 8, \n",
        "                       d_k = cmt_channels[0], \n",
        "                       d_v = cmt_channels[0], \n",
        "                       num_heads = 1, \n",
        "                       R = R, \n",
        "                       in_channels = patch_channels[0],\n",
        "                       attention_type = attention_type) for _ in range(block_layers[0])]\n",
        "    self.stage1 = nn.Sequential(*stage1)\n",
        "\n",
        "    # Stage 2 CMT blocks.\n",
        "    stage2 = [CMTBlock(img_size = img_size // 8,\n",
        "                       k = 4,\n",
        "                       d_k = cmt_channels[1] // 2,\n",
        "                       d_v = cmt_channels[1] // 2,\n",
        "                       num_heads = 2,\n",
        "                       R = R,\n",
        "                       in_channels = patch_channels[1],\n",
        "                       attention_type = attention_type) for _ in range(block_layers[1])]\n",
        "    self.stage2 = nn.Sequential(*stage2)\n",
        "\n",
        "    # Stage 3 CMT blocks.\n",
        "    stage3 = [CMTBlock(img_size = img_size // 16,\n",
        "                       k = 2,\n",
        "                       d_k = cmt_channels[2] // 4,\n",
        "                       d_v = cmt_channels[2] // 4,\n",
        "                       num_heads = 4,\n",
        "                       R = R,\n",
        "                       in_channels = patch_channels[2],\n",
        "                       attention_type = attention_type) for _ in range(block_layers[2])]\n",
        "    self.stage3 = nn.Sequential(*stage3)\n",
        "\n",
        "    # Stage 4 CMT blocks.\n",
        "    stage4 = [CMTBlock(img_size = img_size // 32,\n",
        "                       k = 1,\n",
        "                       d_k = cmt_channels[3] // 8,\n",
        "                       d_v = cmt_channels[3] // 8,\n",
        "                       num_heads = 8,\n",
        "                       R = R,\n",
        "                       in_channels = patch_channels[3],\n",
        "                       attention_type = attention_type) for _ in range(block_layers[3])]\n",
        "    self.stage4 = nn.Sequential(*stage4)\n",
        "\n",
        "    # Global average pooling.\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    # Projection layer.\n",
        "    self.projection = nn.Sequential(\n",
        "      nn.Conv2d(cmt_channels[3], 1280, kernel_size = 1),\n",
        "      nn.ReLU(inplace = True),\n",
        "    )\n",
        "\n",
        "    # Classifier.\n",
        "    self.classifier = nn.Linear(1280, num_class)\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    x = self.stem(x)\n",
        "    x = self.pe1(x)\n",
        "    x = self.stage1(x)\n",
        "    x = self.pe2(x)\n",
        "    x = self.stage2(x)\n",
        "    x = self.pe3(x)\n",
        "    x = self.stage3(x)\n",
        "    x = self.pe4(x)\n",
        "    x = self.stage4(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = self.projection(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    y = self.classifier(x)\n",
        "    return y"
      ],
      "metadata": {
        "id": "yJrsQh-9nZYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains the model.\n",
        "def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, epochs, device = \"cuda\", history = None):\n",
        "\n",
        "  # Metrics.\n",
        "  if history == None: \n",
        "    history = {\"train_loss\": [], \"train_accuracy\": [], \"val_loss\": [], \"val_accuracy\": [], \"lr\": []}\n",
        "\n",
        "  # Iterating over epochs.\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(\"Epoch %2d/%2d:\" % (epoch + 1, epochs))\n",
        "\n",
        "    # Training.\n",
        "    model.train()\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = 0.0\n",
        "    num_train_correct = 0\n",
        "    num_train_examples = 0\n",
        "\n",
        "    # Iterating over mini-batches.\n",
        "    for batch in tqdm(train_loader, position = 0):\n",
        "\n",
        "      # Gradient reset.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Moving x and y to GPU.\n",
        "      x = batch[0].to(device)\n",
        "      y = batch[1].to(device)\n",
        "\n",
        "      # Predictions.\n",
        "      yhat = model(x)\n",
        "\n",
        "      # Loss.\n",
        "      loss = loss_fn(yhat, y)\n",
        "\n",
        "      # Computing the gradient.\n",
        "      loss.backward()\n",
        "\n",
        "      # Updating the parameters.\n",
        "      optimizer.step()\n",
        "\n",
        "      # Updating the metrics.\n",
        "      train_loss += loss.item() * x.shape[0]\n",
        "      num_train_correct += (torch.argmax(yhat, 1) == y).sum().item()\n",
        "      num_train_examples += x.shape[0]\n",
        "\n",
        "    # Computing the epoch's metrics.\n",
        "    train_accuracy = num_train_correct / num_train_examples\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Saving learning rate.\n",
        "    lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    # Updating the learning rate.\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation.\n",
        "    model.eval()\n",
        "\n",
        "    # Metrics.\n",
        "    val_loss = 0.0\n",
        "    num_val_correct = 0\n",
        "    num_val_examples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterating over mini-batches.\n",
        "      for batch in tqdm(val_loader, position = 0):\n",
        "\n",
        "        # Moving x and y to GPU.\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        # Predictions.\n",
        "        yhat = model(x)\n",
        "\n",
        "        # Loss.\n",
        "        loss = loss_fn(yhat, y)\n",
        "\n",
        "        # Updating the metrics.\n",
        "        val_loss += loss.item() * x.shape[0]\n",
        "        num_val_correct += (torch.argmax(yhat, 1) == y).sum().item()\n",
        "        num_val_examples += y.shape[0]\n",
        "\n",
        "      # Computing the epoch's metrics.\n",
        "      val_accuracy = num_val_correct / num_val_examples\n",
        "      val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(\"{}train_loss: {:.4f}, train_accuracy: {:.4f}, val_loss: {:.4f}, val_accuracy: {:.4f}, lr: {:.4e}{}\".format(\"\\n\" if epoch == epochs - 1 else \"\",\n",
        "                                                                                                                      train_loss, \n",
        "                                                                                                                      train_accuracy, \n",
        "                                                                                                                      val_loss, \n",
        "                                                                                                                      val_accuracy,\n",
        "                                                                                                                      lr,\n",
        "                                                                                                                      \"\\n\" if epoch == epochs - 1 else \"\"))\n",
        "\n",
        "    # Appending the epoch's metrics to history lists.\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"train_accuracy\"].append(train_accuracy)\n",
        "    history[\"val_accuracy\"].append(val_accuracy)\n",
        "    history[\"lr\"].append(lr)\n",
        "\n",
        "  # Returning history.\n",
        "  return history"
      ],
      "metadata": {
        "id": "EE9m-lzNr66m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots losses and accuracies.\n",
        "def plot_training(history):\n",
        "\n",
        "  # Computing the x axis array.\n",
        "  x = np.linspace(1, len(history[\"train_loss\"]), len(history[\"train_loss\"]), dtype = int)\n",
        "\n",
        "  # Creating the figure and axes.\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (14, 4))\n",
        "\n",
        "  # Plotting losses.\n",
        "  ax1.plot(x, history[\"train_loss\"])\n",
        "  ax1.plot(x, history[\"val_loss\"])\n",
        "\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_xticks(x)\n",
        "  ax1.legend([\"Training\", \"Testing\"])\n",
        "\n",
        "  # Plotting accuracies.\n",
        "  ax2.plot(x, history[\"train_accuracy\"])\n",
        "  ax2.plot(x, history[\"val_accuracy\"])\n",
        "\n",
        "  ax2.set_ylabel(\"Accuracy\")\n",
        "  ax2.set_xlabel(\"Epoch\")\n",
        "  ax2.set_xticks(x)\n",
        "  ax2.legend([\"Training\", \"Testing\"])\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Yk-QbzFQW69V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads a checkpoint.\n",
        "def load_checkpoint(path, model, optimizer, scheduler):\n",
        "\n",
        "  # Loading checkpoint.\n",
        "  checkpoint = torch.load(path)\n",
        "  model.load_state_dict(checkpoint[\"model\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "  scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "\n",
        "  # Returning checkpoint entries.\n",
        "  return model, optimizer, scheduler, checkpoint[\"history\"]"
      ],
      "metadata": {
        "id": "m303-E_FSBZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightweight MHSA model ($m_1$)"
      ],
      "metadata": {
        "id": "34JWWDLFPwJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model that uses LMHSA.\n",
        "m_1 = CMT(\n",
        "  in_channels = 3,\n",
        "  stem_channels = 16,\n",
        "  cmt_channels = [46, 92, 184, 368],\n",
        "  patch_channels = [46, 92, 184, 368],\n",
        "  block_layers = [2, 2, 10, 2],\n",
        "  R = 3.6,\n",
        "  img_size = 224,\n",
        "  num_class = 10,\n",
        "  attention_type = \"light\"\n",
        ")\n",
        "\n",
        "# Printing number of parameters.\n",
        "print(f\"m_1 has {sum(p.numel() for p in m_1.parameters())} parameters.\")"
      ],
      "metadata": {
        "id": "STqrfOj30eTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model to device.\n",
        "m_1.to(device)"
      ],
      "metadata": {
        "id": "5JqbvDNn1caw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs.\n",
        "EPOCHS = 1\n",
        "\n",
        "# Initial learning rate.\n",
        "LR = 6e-5\n",
        "\n",
        "# Weight decay.\n",
        "WD = 1e-5\n",
        "\n",
        "# Loss function.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer.\n",
        "optimizer = torch.optim.AdamW(m_1.parameters(), lr = LR, weight_decay = WD)\n",
        "\n",
        "# Scheduler.\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS)"
      ],
      "metadata": {
        "id": "30WHWDVvx5Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training.\n",
        "history = train(m_1, optimizer, scheduler, loss_fn, train_loader, val_loader, EPOCHS, device)\n",
        "\n",
        "# Creating a checkpoint.\n",
        "checkpoint = {\n",
        "  \"history\": history,\n",
        "  \"model\": m_1.state_dict(),\n",
        "  \"optimizer\": optimizer.state_dict(),\n",
        "  \"scheduler\": scheduler.state_dict()\n",
        "}\n",
        "\n",
        "# Saving the model.\n",
        "torch.save(checkpoint, \"checkpoint.pt\")"
      ],
      "metadata": {
        "id": "nIOi_J2Ot4Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting losses and accuracies.\n",
        "plot_training(history)"
      ],
      "metadata": {
        "id": "3jbK0PuuXlFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard MHSA model ($m_2$)"
      ],
      "metadata": {
        "id": "Z3tA4hXcRwt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No attention model ($m_3$)"
      ],
      "metadata": {
        "id": "GBo9AEepRzjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Testing"
      ],
      "metadata": {
        "id": "I6gNoTUjPqvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightweight MHSA model ($m_1$)"
      ],
      "metadata": {
        "id": "ClULEfjlPu56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation.\n",
        "m_1.eval()\n",
        "\n",
        "# Predictions and ground truth.\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  # Iterating over mini-batches.\n",
        "  for batch in tqdm(test_loader, position = 0):\n",
        "\n",
        "    # Moving x and y to GPU.\n",
        "    x = batch[0].to(device)\n",
        "    y = batch[1].to(device)\n",
        "\n",
        "    # Predictions.\n",
        "    yhat = m_1(x)\n",
        "\n",
        "    # Updating variables.\n",
        "    y_pred.extend(torch.argmax(yhat, 1).tolist())\n",
        "    y_true.extend(y.tolist())"
      ],
      "metadata": {
        "id": "gRr29r0ASG8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report.\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "g3cCodaVUdpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard MHSA model ($m_2$)"
      ],
      "metadata": {
        "id": "Fkjn8hY6P0tX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No attention model ($m_3$) "
      ],
      "metadata": {
        "id": "Xen_6qHLP4NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References <a name=\"references\"></a>"
      ],
      "metadata": {
        "id": "PHqUD8h6Gzb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu and Yunhe Wang. CMT: Convolutional Neural Networks Meet Vision Transformers, 2021. [https://arxiv.org/abs/2107.06263](https://arxiv.org/abs/2107.06263)."
      ],
      "metadata": {
        "id": "mVKK69jfHAuM"
      }
    }
  ]
}