{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PHqUD8h6Gzb8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <h1> Machine Learning for Computer Vision</h1>\n",
        "    <h2> Project Work </h2>\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Student**: Matteo Donati <br>\n",
        "**Student ID**: 0001032227 <br>\n",
        "**E-mail**: <a href=\"mailto:matteo.donati10@studio.unibo.it\">matteo.donati10@studio.unibo.it</a>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "hgXhyNHqFtSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an unofficial implementation of the paper by Guo et al., (2021)<sup>[[1]](#references)</sup>."
      ],
      "metadata": {
        "id": "mXVx3Q1PGpAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "BFVvggVfZuN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T\n",
        "import multiprocessing as mp\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "SA3HZYkVZrxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "rk5IqtHwZyfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieves the CIFAR10 dataset.\n",
        "def get_cifar10(train_transform = None, test_transform = None):\n",
        "\n",
        "  # Downloading the training set.\n",
        "  train_dataset =  datasets.CIFAR10(\n",
        "    root = \"CIFAR10\",\n",
        "    train = True,\n",
        "    transform = train_transform,\n",
        "    download = True\n",
        "  )\n",
        "\n",
        "  # Downloading the test set.\n",
        "  test_dataset = datasets.CIFAR10(\n",
        "    root = \"CIFAR10\",\n",
        "    train = False,\n",
        "    transform = test_transform,\n",
        "    download = True\n",
        "  )\n",
        "\n",
        "  # Returning the two sets.\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "# Fuction that creates train, validation and test data loaders.\n",
        "def create_data_loaders(train_transform, \n",
        "                        test_transform, \n",
        "                        img_size = 224, \n",
        "                        split = (0.8, 0.2), \n",
        "                        batch_size = 64, \n",
        "                        num_workers = 1):\n",
        "\n",
        "  # Retrieving CIFAR10.\n",
        "  train_dataset, test_dataset = get_cifar10(train_transform, test_transform)\n",
        "\n",
        "  # Splitting train_dataset to create a validation set.\n",
        "  train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, \n",
        "                                                             (int(len(train_dataset) * split[0]), \n",
        "                                                              int(len(train_dataset) * split[1])))\n",
        "\n",
        "  # Train data loader.\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    drop_last = True,\n",
        "    sampler = None\n",
        "  )\n",
        "\n",
        "  # Validation data loader.\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    drop_last = False,\n",
        "    sampler = None\n",
        "  )\n",
        "\n",
        "  # Test data loader.\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    drop_last = False,\n",
        "    sampler = None\n",
        "  )\n",
        "\n",
        "  # Returning the three data loaders.\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "sTkFIuugZxtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training transformations.\n",
        "train_transform = T.Compose([\n",
        "    T.RandomCrop(32, padding = 4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(10),\n",
        "    T.Resize(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean = [0.491, 0.482, 0.447], std = [0.247, 0.243, 0.262])\n",
        "])\n",
        "\n",
        "# Test transformations.\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean = [0.491, 0.482, 0.447], std = [0.247, 0.243, 0.262])\n",
        "])\n",
        "\n",
        "# Creating data loaders.\n",
        "train_loader, valid_loader, test_loader = create_data_loaders(\n",
        "    train_transform,\n",
        "    test_transform,\n",
        "    img_size = 224,\n",
        "    batch_size = 64,\n",
        "    num_workers = mp.cpu_count()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-yP8OHGhPuE",
        "outputId": "86be3e95-1070-4c39-f103-510cdd348028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "bpKz7V32su2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References <a name=\"references\"></a>"
      ],
      "metadata": {
        "id": "PHqUD8h6Gzb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu and Yunhe Wang. CMT: Convolutional Neural Networks Meet Vision Transformers, 2021. [https://arxiv.org/abs/2107.06263](https://arxiv.org/abs/2107.06263)."
      ],
      "metadata": {
        "id": "mVKK69jfHAuM"
      }
    }
  ]
}