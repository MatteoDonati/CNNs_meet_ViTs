{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BFVvggVfZuN1",
        "rk5IqtHwZyfV",
        "axkF_Bd1F_eM",
        "Z3tA4hXcRwt6",
        "GBo9AEepRzjQ",
        "Fkjn8hY6P0tX",
        "Xen_6qHLP4NB",
        "PHqUD8h6Gzb8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0a6e5863ea046dcac7f0adc73bfd687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89020b46996c4d78b5a6dcbf3dc6a708",
              "IPY_MODEL_d18f14d3d20240918cec2a19f822a885",
              "IPY_MODEL_bde27abe62c44454a9e30eaf83be9324"
            ],
            "layout": "IPY_MODEL_333f4bd97002446fb8a5f18cffc05cfc"
          }
        },
        "89020b46996c4d78b5a6dcbf3dc6a708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a0db44276e64297be671ba6d59733cc",
            "placeholder": "​",
            "style": "IPY_MODEL_9ad2eb32cc16436abc7791dfdebeb008",
            "value": "100%"
          }
        },
        "d18f14d3d20240918cec2a19f822a885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c90f03afbbc742d58fc9e5c3c06b8534",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f657c1151b734be7b49f114a3938ef99",
            "value": 170498071
          }
        },
        "bde27abe62c44454a9e30eaf83be9324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeeb43aedb104c99814c4e713006d1d8",
            "placeholder": "​",
            "style": "IPY_MODEL_950bea697e774d6aae214eea0685d011",
            "value": " 170498071/170498071 [00:12&lt;00:00, 11063561.17it/s]"
          }
        },
        "333f4bd97002446fb8a5f18cffc05cfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a0db44276e64297be671ba6d59733cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad2eb32cc16436abc7791dfdebeb008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c90f03afbbc742d58fc9e5c3c06b8534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f657c1151b734be7b49f114a3938ef99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eeeb43aedb104c99814c4e713006d1d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "950bea697e774d6aae214eea0685d011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <h1> Machine Learning for Computer Vision</h1>\n",
        "    <h2> Project Work </h2>\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Student**: Matteo Donati <br>\n",
        "**Student ID**: 0001032227 <br>\n",
        "**E-mail**: <a href=\"mailto:matteo.donati10@studio.unibo.it\">matteo.donati10@studio.unibo.it</a>\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "hgXhyNHqFtSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an unofficial implementation of the paper by Guo et al., (2021)<sup>[[1]](#references)</sup>."
      ],
      "metadata": {
        "id": "mXVx3Q1PGpAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "BFVvggVfZuN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T\n",
        "import multiprocessing as mp\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "SA3HZYkVZrxV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_reproducibility(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "set_reproducibility(0)"
      ],
      "metadata": {
        "id": "Gx8K3KIcgJMI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "rk5IqtHwZyfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieves the CIFAR10 dataset.\n",
        "def get_cifar10(train_transform = None, test_transform = None):\n",
        "\n",
        "  # Downloading the training set.\n",
        "  train_dataset =  datasets.CIFAR10(\n",
        "    root = \"CIFAR10\",\n",
        "    train = True,\n",
        "    transform = train_transform,\n",
        "    download = True\n",
        "  )\n",
        "\n",
        "  # Downloading the test set.\n",
        "  test_dataset = datasets.CIFAR10(\n",
        "    root = \"CIFAR10\",\n",
        "    train = False,\n",
        "    transform = test_transform,\n",
        "    download = True\n",
        "  )\n",
        "\n",
        "  # Returning the two sets.\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "# Fuction that creates train, validation and test data loaders.\n",
        "def create_data_loaders(train_transform, \n",
        "                        test_transform, \n",
        "                        img_size = 224, \n",
        "                        split = (0.8, 0.2), \n",
        "                        batch_size = 32, \n",
        "                        num_workers = 1):\n",
        "\n",
        "  # Retrieving CIFAR10.\n",
        "  train_dataset, test_dataset = get_cifar10(train_transform, test_transform)\n",
        "\n",
        "  # Splitting train_dataset to create a validation set.\n",
        "  train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, \n",
        "                                                             (int(len(train_dataset) * split[0]), \n",
        "                                                              int(len(train_dataset) * split[1])))\n",
        "\n",
        "  # Train data loader.\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    drop_last = True,\n",
        "    sampler = None\n",
        "  )\n",
        "\n",
        "  # Validation data loader.\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    drop_last = False,\n",
        "    sampler = None\n",
        "  )\n",
        "\n",
        "  # Test data loader.\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "    drop_last = False,\n",
        "    sampler = None\n",
        "  )\n",
        "\n",
        "  # Returning the three data loaders.\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "sTkFIuugZxtD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training transformations.\n",
        "train_transform = T.Compose([\n",
        "    T.RandomCrop(32, padding = 4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(10),\n",
        "    T.Resize(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean = [0.491, 0.482, 0.447], std = [0.247, 0.243, 0.262])\n",
        "])\n",
        "\n",
        "# Test transformations.\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean = [0.491, 0.482, 0.447], std = [0.247, 0.243, 0.262])\n",
        "])\n",
        "\n",
        "# Creating data loaders.\n",
        "train_loader, val_loader, test_loader = create_data_loaders(\n",
        "    train_transform,\n",
        "    test_transform,\n",
        "    img_size = 224,\n",
        "    batch_size = 32,\n",
        "    num_workers = mp.cpu_count()\n",
        ")"
      ],
      "metadata": {
        "id": "2-yP8OHGhPuE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "b0a6e5863ea046dcac7f0adc73bfd687",
            "89020b46996c4d78b5a6dcbf3dc6a708",
            "d18f14d3d20240918cec2a19f822a885",
            "bde27abe62c44454a9e30eaf83be9324",
            "333f4bd97002446fb8a5f18cffc05cfc",
            "2a0db44276e64297be671ba6d59733cc",
            "9ad2eb32cc16436abc7791dfdebeb008",
            "c90f03afbbc742d58fc9e5c3c06b8534",
            "f657c1151b734be7b49f114a3938ef99",
            "eeeb43aedb104c99814c4e713006d1d8",
            "950bea697e774d6aae214eea0685d011"
          ]
        },
        "outputId": "f0522170-6b35-4bac-af99-0eb6d769bc8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0a6e5863ea046dcac7f0adc73bfd687"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting CIFAR10/cifar-10-python.tar.gz to CIFAR10\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Definition and Training"
      ],
      "metadata": {
        "id": "bpKz7V32su2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities"
      ],
      "metadata": {
        "id": "axkF_Bd1F_eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing between GPU and CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "oaT-QCAapgdk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stem module of the CMT architecture.\n",
        "class Stem(nn.Module):\n",
        "    \n",
        "  # Constructor. Requires number of input and output channels.\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Conv 3 x 3, stride 2.\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # Conv 3 x 3. \n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # Conv 3 x 3.\n",
        "    self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # GELU activation.\n",
        "    self.gelu = nn.GELU()\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.gelu(x)\n",
        "    y = self.bn3(x)\n",
        "    return y"
      ],
      "metadata": {
        "id": "bAsbc7UBsoy3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch embedding module of the CMT architecture.\n",
        "class PatchEmbedding(nn.Module):\n",
        "\n",
        "  # Constructor. Requires number of input and output channels.\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Conv 2 x 2, stride 2.\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    _, c, h, w = x.size()\n",
        "    y = torch.nn.functional.layer_norm(x, (c, h, w))\n",
        "    return y"
      ],
      "metadata": {
        "id": "7BVXE1Jw0HoY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Perception Unit: LPU(X) = DWConv(X) + X.\n",
        "class LPU(nn.Module):\n",
        "\n",
        "  # Constructor.\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Depthwise convolution.\n",
        "    self.dwconv = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, groups = in_channels)\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    y = self.dwconv(x) + x\n",
        "    return y"
      ],
      "metadata": {
        "id": "OFDPglv5iNZ1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lightweight Multi-Head-Self-Attention.\n",
        "class LMHSA(nn.Module):\n",
        "    \n",
        "  # Constructor.\n",
        "  def __init__(self, input_size, channels, d_k, d_v, stride, heads):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Depthwise convolutions.\n",
        "    self.dwconv = nn.Conv2d(channels, channels, kernel_size = stride, stride = stride, groups = channels)\n",
        "\n",
        "    # Projection matrices.\n",
        "    self.fc_q = nn.Linear(channels, heads * d_k)\n",
        "    self.fc_k = nn.Linear(channels, heads * d_k)\n",
        "    self.fc_v = nn.Linear(channels, heads * d_v)\n",
        "    self.fc_o = nn.Linear(heads * d_k, channels)\n",
        "\n",
        "    self.channels = channels\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.stride = stride\n",
        "    self.heads = heads\n",
        "\n",
        "    # Relative position bias to each self-attention module.\n",
        "    self.B = nn.Parameter(torch.randn(1, self.heads, input_size ** 2, (input_size // stride) ** 2), requires_grad = True)\n",
        "    \n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Extracting shape from input signal.\n",
        "    b, c, h, w = x.shape\n",
        "\n",
        "    # Reshaping and permuting x. Final shape is (b, h * w, c).\n",
        "    x_reshape = x.view(b, c, h * w).permute(0, 2, 1)\n",
        "\n",
        "    # Layer norm.\n",
        "    x_reshape = torch.nn.functional.layer_norm(x_reshape, (b, h * w, c))\n",
        "\n",
        "    # Getting queries by applying the fc_q linear projection.\n",
        "    q = self.fc_q(x_reshape)\n",
        "\n",
        "    # Reshaping and permuting the queries. Final shape is (b, heads, n = h * w, d_k). \n",
        "    q = q.view(b, h * w, self.heads, self.d_k).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Applying depthwise conv to x.\n",
        "    k = self.dwconv(x)\n",
        "\n",
        "    # Extracting shape of keys.\n",
        "    k_b, k_c, k_h, k_w = k.shape\n",
        "\n",
        "    # Reshaping and permuting keys. Final shape is (k_b, k_h * k_w, k_c).\n",
        "    k = k.view(k_b, k_c, k_h * k_w).permute(0, 2, 1).contiguous()\n",
        "\n",
        "    # Projecting through fc_k.\n",
        "    k = self.fc_k(k)\n",
        "\n",
        "    # Reshaping and permuting the keys. Final shape is (k_b, heads, k_h * k_w, d_k).\n",
        "    k = k.view(k_b, k_h * k_w, self.heads, self.d_k).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Applying depthwise conv to x.\n",
        "    v = self.dwconv(x)\n",
        "\n",
        "    # Extracting shape of values.\n",
        "    v_b, v_c, v_h, v_w = v.shape\n",
        "\n",
        "    # Reshaping and permuting values. Final shape is (v_b, v_h * v_w, v_c).\n",
        "    v = v.view(v_b, v_c, v_h * v_w).permute(0, 2, 1).contiguous()\n",
        "\n",
        "    # Projecting through fc_v.\n",
        "    v = self.fc_v(v)\n",
        "\n",
        "    # Reshaping and permuting the keys. Final shape is (v_b, heads, v_h * v_w, d_v).\n",
        "    v = v.view(v_b, v_h * v_w, self.heads, self.d_v).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Computing softmax((Q K') / sqrt(d_k) + B). Final shape is (b, heads, n = h * w, k_h * k_w).\n",
        "    attention = torch.einsum(\"... i d, ... j d -> ... i j\", q, k) * (self.d_k ** -0.5)\n",
        "    attention = attention + self.B\n",
        "    attention = torch.softmax(attention, dim = -1)\n",
        "\n",
        "    # Applying attention scores to values by taking the dot product.\n",
        "    tmp = torch.matmul(attention, v).permute(0, 2, 1, 3)\n",
        "\n",
        "    # Permuting the result. Final shape is (b, n = h * w, heads, d_v).\n",
        "    tmp = tmp.contiguous().view(b, h * w, self.heads * self.d_v)\n",
        "\n",
        "    # Projecting using fc_o and reshaping. Final shape is (b, c, h, w).\n",
        "    tmp = self.fc_o(tmp).view(b, self.channels, h, w)\n",
        "\n",
        "    # Returning tmp + x (skip connection).\n",
        "    return tmp + x"
      ],
      "metadata": {
        "id": "mwVBiJHhlUY0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Multi-Head-Self-Attention.\n",
        "class MHSA(nn.Module):\n",
        "    \n",
        "  # Constructor.\n",
        "  def __init__(self, input_size, channels, d_k, d_v, stride, heads):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Projection matrices.\n",
        "    self.fc_q = nn.Linear(channels, heads * d_k)\n",
        "    self.fc_k = nn.Linear(channels, heads * d_k)\n",
        "    self.fc_v = nn.Linear(channels, heads * d_v)\n",
        "    self.fc_o = nn.Linear(heads * d_k, channels)\n",
        "\n",
        "    self.channels = channels\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.stride = stride\n",
        "    self.heads = heads\n",
        "    \n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Extracting shape from input signal.\n",
        "    b, c, h, w = x.shape\n",
        "\n",
        "    # Reshaping and permuting x. Final shape is (b, h * w, c).\n",
        "    x_reshape = x.view(b, c, h * w).permute(0, 2, 1)\n",
        "\n",
        "    # Layer norm.\n",
        "    x_reshape = torch.nn.functional.layer_norm(x_reshape, (b, h * w, c))\n",
        "\n",
        "    # Getting queries by applying the fc_q linear projection.\n",
        "    q = self.fc_q(x_reshape)\n",
        "\n",
        "    # Reshaping and permuting the queries. Final shape is (b, heads, n = h * w, d_k). \n",
        "    q = q.view(b, h * w, self.heads, self.d_k).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Projecting through fc_k.\n",
        "    k = self.fc_k(x_reshape)\n",
        "\n",
        "    # Reshaping and permuting the keys. Final shape is (b, heads, h * w, d_k).\n",
        "    k = k.view(b, h * w, self.heads, self.d_k).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Projecting through fc_v.\n",
        "    v = self.fc_v(x_reshape)\n",
        "\n",
        "    # Reshaping and permuting the keys. Final shape is (b, heads, h * w, d_v).\n",
        "    v = v.view(b, h * w, self.heads, self.d_v).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    # Computing softmax((Q K) / sqrt(d_k)).\n",
        "    attention = torch.einsum(\"... i d, ... j d -> ... i j\", q, k) * (self.d_k ** -0.5)\n",
        "    attention = torch.softmax(attention, dim = -1)\n",
        "\n",
        "    # Applying attention scores to values by taking the dot product.\n",
        "    tmp = torch.matmul(attention, v).permute(0, 2, 1, 3)\n",
        "\n",
        "    # Permuting the result. Final shape is (b, n = h * w, heads, d_v).\n",
        "    tmp = tmp.contiguous().view(b, h * w, self.heads * self.d_v)\n",
        "\n",
        "    # Projecting using fc_o and reshaping. Final shape is (b, c, h, w).\n",
        "    tmp = self.fc_o(tmp).view(b, self.channels, h, w)\n",
        "\n",
        "    # Returning tmp + x (skip connection).\n",
        "    return tmp + x"
      ],
      "metadata": {
        "id": "Hdl8rdu8lb-A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverted Residual Feed-forward Network: IRFFN(X) = Conv(F(Conv(X))), F(X) = DWConv(X) + X.\n",
        "class IRFFN(nn.Module):\n",
        "\n",
        "  # Constructor.\n",
        "  def __init__(self, in_channels, R):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Number of channels after expansion.\n",
        "    out_channels = int(in_channels * R)\n",
        "\n",
        "    # Conv 1 x 1.\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # Depthwise convolution.\n",
        "    self.dwconv = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, groups = out_channels)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # Conv 1 x 1.\n",
        "    self.conv2 = nn.Conv2d(out_channels, in_channels, kernel_size = 1)\n",
        "    self.bn3 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "    # GELU activation.\n",
        "    self.gelu = nn.GELU()\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    tmp = self.conv1(x)\n",
        "    tmp = self.bn1(tmp)\n",
        "    tmp = self.gelu(tmp)\n",
        "    tmp = self.dwconv(tmp)\n",
        "    tmp = self.bn2(tmp)\n",
        "    tmp = self.gelu(tmp)\n",
        "    tmp = self.conv2(tmp)\n",
        "    tmp = self.bn3(tmp)\n",
        "    y = x + tmp\n",
        "    return y"
      ],
      "metadata": {
        "id": "WQRfI_lQlYoa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CMT block of the CMT architecture.\n",
        "class CMTBlock(nn.Module):\n",
        "\n",
        "  # Constructor. By default it loads the CMT-Ti configuration.\n",
        "  def __init__(self, img_size, k, d_k, d_v, num_heads, R = 3.6, in_channels = 46, attention_type = \"light\"):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Local Perception Unit.\n",
        "    self.lpu = LPU(in_channels, in_channels)\n",
        "\n",
        "    # Lightweight MHSA.\n",
        "    if attention_type == \"light\":\n",
        "      self.mhsa = LMHSA(img_size, in_channels, d_k, d_v, k, num_heads)\n",
        "    \n",
        "    # Lightweight MHSA.\n",
        "    elif attention_type == \"standard\":\n",
        "      self.mhsa = MHSA(img_size, in_channels, d_k, d_v, k, num_heads)\n",
        "    \n",
        "    # No attention.\n",
        "    else:\n",
        "      self.mhsa = None\n",
        "\n",
        "    # Inverted Residual FFN.\n",
        "    self.irffn = IRFFN(in_channels, R)\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    x = self.lpu(x)\n",
        "    if self.mhsa != None: x = self.mhsa(x)\n",
        "    y = self.irffn(x)\n",
        "    return y"
      ],
      "metadata": {
        "id": "sGf0VE9hhtVh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CMT architecture.\n",
        "class CMT(nn.Module):\n",
        "\n",
        "  # Constructor. By default it loads the CMT-Ti configuration.\n",
        "  def __init__(self,\n",
        "    in_channels = 3,\n",
        "    stem_channels = 16,\n",
        "    cmt_channels = [46, 92, 184, 368],\n",
        "    patch_channels = [46, 92, 184, 368],\n",
        "    block_layers = [2, 2, 10, 2],\n",
        "    R = 3.6,\n",
        "    img_size = 224,\n",
        "    num_class = 10,\n",
        "    attention_type = \"light\"\n",
        "  ):\n",
        "\n",
        "    super(CMT, self).__init__()\n",
        "\n",
        "    # Stem layer\n",
        "    self.stem = Stem(in_channels, stem_channels)\n",
        "\n",
        "    # Patch Aggregation Layer\n",
        "    self.pe1 = PatchEmbedding(stem_channels, patch_channels[0])\n",
        "    self.pe2 = PatchEmbedding(patch_channels[0], patch_channels[1])\n",
        "    self.pe3 = PatchEmbedding(patch_channels[1], patch_channels[2])\n",
        "    self.pe4 = PatchEmbedding(patch_channels[2], patch_channels[3])\n",
        "\n",
        "    # Stage 1 CMT blocks.\n",
        "    stage1 = [CMTBlock(img_size = img_size // 4, \n",
        "                       k = 8, \n",
        "                       d_k = cmt_channels[0], \n",
        "                       d_v = cmt_channels[0], \n",
        "                       num_heads = 1, \n",
        "                       R = R, \n",
        "                       in_channels = patch_channels[0],\n",
        "                       attention_type = attention_type) for _ in range(block_layers[0])]\n",
        "    self.stage1 = nn.Sequential(*stage1)\n",
        "\n",
        "    # Stage 2 CMT blocks.\n",
        "    stage2 = [CMTBlock(img_size = img_size // 8,\n",
        "                       k = 4,\n",
        "                       d_k = cmt_channels[1] // 2,\n",
        "                       d_v = cmt_channels[1] // 2,\n",
        "                       num_heads = 2,\n",
        "                       R = R,\n",
        "                       in_channels = patch_channels[1],\n",
        "                       attention_type = attention_type) for _ in range(block_layers[1])]\n",
        "    self.stage2 = nn.Sequential(*stage2)\n",
        "\n",
        "    # Stage 3 CMT blocks.\n",
        "    stage3 = [CMTBlock(img_size = img_size // 16,\n",
        "                       k = 2,\n",
        "                       d_k = cmt_channels[2] // 4,\n",
        "                       d_v = cmt_channels[2] // 4,\n",
        "                       num_heads = 4,\n",
        "                       R = R,\n",
        "                       in_channels = patch_channels[2],\n",
        "                       attention_type = attention_type) for _ in range(block_layers[2])]\n",
        "    self.stage3 = nn.Sequential(*stage3)\n",
        "\n",
        "    # Stage 4 CMT blocks.\n",
        "    stage4 = [CMTBlock(img_size = img_size // 32,\n",
        "                       k = 1,\n",
        "                       d_k = cmt_channels[3] // 8,\n",
        "                       d_v = cmt_channels[3] // 8,\n",
        "                       num_heads = 8,\n",
        "                       R = R,\n",
        "                       in_channels = patch_channels[3],\n",
        "                       attention_type = attention_type) for _ in range(block_layers[3])]\n",
        "    self.stage4 = nn.Sequential(*stage4)\n",
        "\n",
        "    # Global average pooling.\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    # Projection layer.\n",
        "    self.projection = nn.Sequential(\n",
        "      nn.Conv2d(cmt_channels[3], 1280, kernel_size = 1),\n",
        "      nn.ReLU(inplace = True),\n",
        "    )\n",
        "\n",
        "    # Classifier.\n",
        "    self.classifier = nn.Linear(1280, num_class)\n",
        "\n",
        "  # Forward pass.\n",
        "  def forward(self, x):\n",
        "    x = self.stem(x)\n",
        "    x = self.pe1(x)\n",
        "    x = self.stage1(x)\n",
        "    x = self.pe2(x)\n",
        "    x = self.stage2(x)\n",
        "    x = self.pe3(x)\n",
        "    x = self.stage3(x)\n",
        "    x = self.pe4(x)\n",
        "    x = self.stage4(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = self.projection(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    y = self.classifier(x)\n",
        "    return y"
      ],
      "metadata": {
        "id": "yJrsQh-9nZYH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains the model.\n",
        "def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, epochs, device = \"cuda\", history = None):\n",
        "\n",
        "  # Metrics.\n",
        "  if history == None: \n",
        "    history = {\"train_loss\": [], \"train_accuracy\": [], \"val_loss\": [], \"val_accuracy\": [], \"lr\": []}\n",
        "\n",
        "  # Iterating over epochs.\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(\"Epoch %2d/%2d:\" % (epoch + 1, epochs))\n",
        "\n",
        "    # Training.\n",
        "    model.train()\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = 0.0\n",
        "    num_train_correct = 0\n",
        "    num_train_examples = 0\n",
        "\n",
        "    # Iterating over mini-batches.\n",
        "    for batch in tqdm(train_loader, desc = \"Training\", position = 0):\n",
        "\n",
        "      # Gradient reset.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Moving x and y to GPU.\n",
        "      x = batch[0].to(device)\n",
        "      y = batch[1].to(device)\n",
        "\n",
        "      # Predictions.\n",
        "      yhat = model(x)\n",
        "\n",
        "      # Loss.\n",
        "      loss = loss_fn(yhat, y)\n",
        "\n",
        "      # Computing the gradient.\n",
        "      loss.backward()\n",
        "\n",
        "      # Updating the parameters.\n",
        "      optimizer.step()\n",
        "\n",
        "      # Updating the metrics.\n",
        "      train_loss += loss.item() * x.shape[0]\n",
        "      num_train_correct += (torch.argmax(yhat, 1) == y).sum().item()\n",
        "      num_train_examples += x.shape[0]\n",
        "\n",
        "    # Computing the epoch's metrics.\n",
        "    train_accuracy = num_train_correct / num_train_examples\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Saving learning rate.\n",
        "    lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    # Updating the learning rate.\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation.\n",
        "    model.eval()\n",
        "\n",
        "    # Metrics.\n",
        "    val_loss = 0.0\n",
        "    num_val_correct = 0\n",
        "    num_val_examples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterating over mini-batches.\n",
        "      for batch in tqdm(val_loader, desc = \"Validation\", position = 0):\n",
        "\n",
        "        # Moving x and y to GPU.\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        # Predictions.\n",
        "        yhat = model(x)\n",
        "\n",
        "        # Loss.\n",
        "        loss = loss_fn(yhat, y)\n",
        "\n",
        "        # Updating the metrics.\n",
        "        val_loss += loss.item() * x.shape[0]\n",
        "        num_val_correct += (torch.argmax(yhat, 1) == y).sum().item()\n",
        "        num_val_examples += y.shape[0]\n",
        "\n",
        "      # Computing the epoch's metrics.\n",
        "      val_accuracy = num_val_correct / num_val_examples\n",
        "      val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(\"{}train_loss: {:.4f}, train_accuracy: {:.4f}, val_loss: {:.4f}, val_accuracy: {:.4f}, lr: {:.4e}{}\".format(\"\\n\" if epoch == epochs - 1 else \"\",\n",
        "                                                                                                                      train_loss, \n",
        "                                                                                                                      train_accuracy, \n",
        "                                                                                                                      val_loss, \n",
        "                                                                                                                      val_accuracy,\n",
        "                                                                                                                      lr,\n",
        "                                                                                                                      \"\\n\" if epoch != epochs - 1 else \"\"))\n",
        "\n",
        "    # Appending the epoch's metrics to history lists.\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"train_accuracy\"].append(train_accuracy)\n",
        "    history[\"val_accuracy\"].append(val_accuracy)\n",
        "    history[\"lr\"].append(lr)\n",
        "\n",
        "  # Returning history.\n",
        "  return history"
      ],
      "metadata": {
        "id": "EE9m-lzNr66m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots losses and accuracies.\n",
        "def plot_training(history):\n",
        "\n",
        "  # Computing the x axis array.\n",
        "  x = np.linspace(1, len(history[\"train_loss\"]), len(history[\"train_loss\"]), dtype = int)\n",
        "\n",
        "  # Creating the figure and axes.\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (14, 4))\n",
        "\n",
        "  # Plotting losses.\n",
        "  ax1.plot(x, history[\"train_loss\"])\n",
        "  ax1.plot(x, history[\"val_loss\"])\n",
        "\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_xticks(x)\n",
        "  ax1.legend([\"Training\", \"Testing\"])\n",
        "\n",
        "  # Plotting accuracies.\n",
        "  ax2.plot(x, history[\"train_accuracy\"])\n",
        "  ax2.plot(x, history[\"val_accuracy\"])\n",
        "\n",
        "  ax2.set_ylabel(\"Accuracy\")\n",
        "  ax2.set_xlabel(\"Epoch\")\n",
        "  ax2.set_xticks(x)\n",
        "  ax2.legend([\"Training\", \"Testing\"])\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Yk-QbzFQW69V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests a model.\n",
        "def test(model, test_loader):\n",
        "  \n",
        "  # Tetsing.\n",
        "  model.eval()\n",
        "\n",
        "  # Predictions and ground truth.\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # Iterating over mini-batches.\n",
        "    for batch in tqdm(test_loader, desc = \"Testing\", position = 0):\n",
        "\n",
        "      # Moving x and y to GPU.\n",
        "      x = batch[0].to(device)\n",
        "      y = batch[1].to(device)\n",
        "\n",
        "      # Predictions.\n",
        "      yhat = model(x)\n",
        "\n",
        "      # Updating variables.\n",
        "      y_pred.extend(torch.argmax(yhat, 1).tolist())\n",
        "      y_true.extend(y.tolist())\n",
        "\n",
        "  # Returning pred and true.\n",
        "  return y_pred, y_true"
      ],
      "metadata": {
        "id": "Venh9VZaGkjN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads a checkpoint.\n",
        "def load_checkpoint(path, model, optimizer, scheduler):\n",
        "\n",
        "  # Loading checkpoint.\n",
        "  checkpoint = torch.load(path)\n",
        "  model.load_state_dict(checkpoint[\"model\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "  scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "\n",
        "  # Returning checkpoint entries.\n",
        "  return model, optimizer, scheduler, checkpoint[\"history\"]"
      ],
      "metadata": {
        "id": "m303-E_FSBZO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightweight MHSA model ($m_1$)"
      ],
      "metadata": {
        "id": "34JWWDLFPwJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model that uses LMHSA.\n",
        "m_1 = CMT(\n",
        "  in_channels = 3,\n",
        "  stem_channels = 16,\n",
        "  cmt_channels = [46, 92, 184, 368],\n",
        "  patch_channels = [46, 92, 184, 368],\n",
        "  block_layers = [2, 2, 10, 2],\n",
        "  R = 3.6,\n",
        "  img_size = 224,\n",
        "  num_class = 10,\n",
        "  attention_type = \"light\")\n",
        "\n",
        "# Printing number of parameters.\n",
        "print(f\"m_1 has {sum(p.numel() for p in m_1.parameters())} parameters.\")"
      ],
      "metadata": {
        "id": "STqrfOj30eTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8a6708-1cb7-4eac-e070-6a8b20003628"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m_1 has 9014696 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model to device.\n",
        "m_1.to(device)"
      ],
      "metadata": {
        "id": "5JqbvDNn1caw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5da6c6-5a75-4de9-cb84-37317a0f72d5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CMT(\n",
              "  (stem): Stem(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (gelu): GELU(approximate='none')\n",
              "  )\n",
              "  (pe1): PatchEmbedding(\n",
              "    (conv): Conv2d(16, 46, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (pe2): PatchEmbedding(\n",
              "    (conv): Conv2d(46, 92, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (pe3): PatchEmbedding(\n",
              "    (conv): Conv2d(92, 184, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (pe4): PatchEmbedding(\n",
              "    (conv): Conv2d(184, 368, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (stage1): Sequential(\n",
              "    (0): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(46, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(46, 46, kernel_size=(8, 8), stride=(8, 8), groups=46)\n",
              "        (fc_q): Linear(in_features=46, out_features=46, bias=True)\n",
              "        (fc_k): Linear(in_features=46, out_features=46, bias=True)\n",
              "        (fc_v): Linear(in_features=46, out_features=46, bias=True)\n",
              "        (fc_o): Linear(in_features=46, out_features=46, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(46, 165, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(165, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(165, 165, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=165)\n",
              "        (bn2): BatchNorm2d(165, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(165, 46, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (1): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(46, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(46, 46, kernel_size=(8, 8), stride=(8, 8), groups=46)\n",
              "        (fc_q): Linear(in_features=46, out_features=46, bias=True)\n",
              "        (fc_k): Linear(in_features=46, out_features=46, bias=True)\n",
              "        (fc_v): Linear(in_features=46, out_features=46, bias=True)\n",
              "        (fc_o): Linear(in_features=46, out_features=46, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(46, 165, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(165, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(165, 165, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=165)\n",
              "        (bn2): BatchNorm2d(165, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(165, 46, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (stage2): Sequential(\n",
              "    (0): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(92, 92, kernel_size=(4, 4), stride=(4, 4), groups=92)\n",
              "        (fc_q): Linear(in_features=92, out_features=92, bias=True)\n",
              "        (fc_k): Linear(in_features=92, out_features=92, bias=True)\n",
              "        (fc_v): Linear(in_features=92, out_features=92, bias=True)\n",
              "        (fc_o): Linear(in_features=92, out_features=92, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(92, 331, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(331, 331, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=331)\n",
              "        (bn2): BatchNorm2d(331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(331, 92, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (1): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(92, 92, kernel_size=(4, 4), stride=(4, 4), groups=92)\n",
              "        (fc_q): Linear(in_features=92, out_features=92, bias=True)\n",
              "        (fc_k): Linear(in_features=92, out_features=92, bias=True)\n",
              "        (fc_v): Linear(in_features=92, out_features=92, bias=True)\n",
              "        (fc_o): Linear(in_features=92, out_features=92, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(92, 331, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(331, 331, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=331)\n",
              "        (bn2): BatchNorm2d(331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(331, 92, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (stage3): Sequential(\n",
              "    (0): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (1): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (2): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (3): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (4): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (5): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (6): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (7): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (8): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (9): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(184, 184, kernel_size=(2, 2), stride=(2, 2), groups=184)\n",
              "        (fc_q): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_k): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_v): Linear(in_features=184, out_features=184, bias=True)\n",
              "        (fc_o): Linear(in_features=184, out_features=184, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(184, 662, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(662, 662, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=662)\n",
              "        (bn2): BatchNorm2d(662, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(662, 184, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (stage4): Sequential(\n",
              "    (0): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=368)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), groups=368)\n",
              "        (fc_q): Linear(in_features=368, out_features=368, bias=True)\n",
              "        (fc_k): Linear(in_features=368, out_features=368, bias=True)\n",
              "        (fc_v): Linear(in_features=368, out_features=368, bias=True)\n",
              "        (fc_o): Linear(in_features=368, out_features=368, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(368, 1324, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(1324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(1324, 1324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1324)\n",
              "        (bn2): BatchNorm2d(1324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(1324, 368, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (1): CMTBlock(\n",
              "      (lpu): LPU(\n",
              "        (dwconv): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=368)\n",
              "      )\n",
              "      (mhsa): LMHSA(\n",
              "        (dwconv): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), groups=368)\n",
              "        (fc_q): Linear(in_features=368, out_features=368, bias=True)\n",
              "        (fc_k): Linear(in_features=368, out_features=368, bias=True)\n",
              "        (fc_v): Linear(in_features=368, out_features=368, bias=True)\n",
              "        (fc_o): Linear(in_features=368, out_features=368, bias=True)\n",
              "      )\n",
              "      (irffn): IRFFN(\n",
              "        (conv1): Conv2d(368, 1324, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(1324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dwconv): Conv2d(1324, 1324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1324)\n",
              "        (bn2): BatchNorm2d(1324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(1324, 368, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn3): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (projection): Sequential(\n",
              "    (0): Conv2d(368, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=1280, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs.\n",
        "EPOCHS = 25\n",
        "\n",
        "# Initial learning rate.\n",
        "LR = 6e-5\n",
        "\n",
        "# Weight decay.\n",
        "WD = 1e-5\n",
        "\n",
        "# Loss function.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer.\n",
        "optimizer = torch.optim.AdamW(m_1.parameters(), lr = LR, weight_decay = WD)\n",
        "\n",
        "# Scheduler.\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS)"
      ],
      "metadata": {
        "id": "30WHWDVvx5Th"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training.\n",
        "history = train(m_1, optimizer, scheduler, loss_fn, train_loader, val_loader, EPOCHS, device)\n",
        "\n",
        "# Creating a checkpoint.\n",
        "checkpoint = {\n",
        "  \"history\": history,\n",
        "  \"model\": m_1.state_dict(),\n",
        "  \"optimizer\": optimizer.state_dict(),\n",
        "  \"scheduler\": scheduler.state_dict()\n",
        "}\n",
        "\n",
        "# Saving the model.\n",
        "torch.save(checkpoint, \"checkpoint1.pt\")"
      ],
      "metadata": {
        "id": "nIOi_J2Ot4Oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9217b84b-147e-42ce-ac6a-6cc716ec6f4c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:40<00:00,  2.40it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.4419, train_accuracy: 0.4693, val_loss: 1.1494, val_accuracy: 0.5843, lr: 6.0000e-05\n",
            "\n",
            "Epoch  2/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:39<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.0438, train_accuracy: 0.6272, val_loss: 0.9299, val_accuracy: 0.6743, lr: 5.9763e-05\n",
            "\n",
            "Epoch  3/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.8683, train_accuracy: 0.6926, val_loss: 0.8188, val_accuracy: 0.7166, lr: 5.9057e-05\n",
            "\n",
            "Epoch  4/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.7362, train_accuracy: 0.7399, val_loss: 0.6910, val_accuracy: 0.7509, lr: 5.7893e-05\n",
            "\n",
            "Epoch  5/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:39<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.6461, train_accuracy: 0.7729, val_loss: 0.6498, val_accuracy: 0.7783, lr: 5.6289e-05\n",
            "\n",
            "Epoch  6/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.5720, train_accuracy: 0.8007, val_loss: 0.5892, val_accuracy: 0.7938, lr: 5.4271e-05\n",
            "\n",
            "Epoch  7/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:39<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.5209, train_accuracy: 0.8195, val_loss: 0.5647, val_accuracy: 0.8024, lr: 5.1869e-05\n",
            "\n",
            "Epoch  8/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:39<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.4774, train_accuracy: 0.8351, val_loss: 0.5312, val_accuracy: 0.8157, lr: 4.9123e-05\n",
            "\n",
            "Epoch  9/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:39<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.4326, train_accuracy: 0.8493, val_loss: 0.5117, val_accuracy: 0.8228, lr: 4.6075e-05\n",
            "\n",
            "Epoch 10/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.4019, train_accuracy: 0.8584, val_loss: 0.4925, val_accuracy: 0.8333, lr: 4.2773e-05\n",
            "\n",
            "Epoch 11/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.3702, train_accuracy: 0.8712, val_loss: 0.4606, val_accuracy: 0.8375, lr: 3.9271e-05\n",
            "\n",
            "Epoch 12/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:37<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.3387, train_accuracy: 0.8825, val_loss: 0.4659, val_accuracy: 0.8401, lr: 3.5621e-05\n",
            "\n",
            "Epoch 13/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.3109, train_accuracy: 0.8916, val_loss: 0.4557, val_accuracy: 0.8450, lr: 3.1884e-05\n",
            "\n",
            "Epoch 14/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:37<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.2853, train_accuracy: 0.8992, val_loss: 0.4529, val_accuracy: 0.8478, lr: 2.8116e-05\n",
            "\n",
            "Epoch 15/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:37<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.2636, train_accuracy: 0.9081, val_loss: 0.4407, val_accuracy: 0.8553, lr: 2.4379e-05\n",
            "\n",
            "Epoch 16/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.2443, train_accuracy: 0.9149, val_loss: 0.4151, val_accuracy: 0.8627, lr: 2.0729e-05\n",
            "\n",
            "Epoch 17/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:37<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.2251, train_accuracy: 0.9211, val_loss: 0.4174, val_accuracy: 0.8619, lr: 1.7227e-05\n",
            "\n",
            "Epoch 18/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.2098, train_accuracy: 0.9258, val_loss: 0.4249, val_accuracy: 0.8623, lr: 1.3925e-05\n",
            "\n",
            "Epoch 19/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:35<00:00,  8.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.1927, train_accuracy: 0.9326, val_loss: 0.4107, val_accuracy: 0.8699, lr: 1.0877e-05\n",
            "\n",
            "Epoch 20/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.1849, train_accuracy: 0.9358, val_loss: 0.4029, val_accuracy: 0.8702, lr: 8.1309e-06\n",
            "\n",
            "Epoch 21/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:38<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.1721, train_accuracy: 0.9399, val_loss: 0.4005, val_accuracy: 0.8723, lr: 5.7295e-06\n",
            "\n",
            "Epoch 22/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:37<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.1648, train_accuracy: 0.9428, val_loss: 0.3990, val_accuracy: 0.8715, lr: 3.7108e-06\n",
            "\n",
            "Epoch 23/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:37<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:35<00:00,  8.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.1621, train_accuracy: 0.9444, val_loss: 0.3982, val_accuracy: 0.8705, lr: 2.1067e-06\n",
            "\n",
            "Epoch 24/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:37<00:00,  2.42it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 0.1569, train_accuracy: 0.9467, val_loss: 0.3990, val_accuracy: 0.8723, lr: 9.4251e-07\n",
            "\n",
            "Epoch 25/25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1250/1250 [08:37<00:00,  2.41it/s]\n",
            "Validation: 100%|██████████| 313/313 [00:36<00:00,  8.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train_loss: 0.1552, train_accuracy: 0.9464, val_loss: 0.3920, val_accuracy: 0.8744, lr: 2.3656e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting losses and accuracies.\n",
        "plot_training(history)"
      ],
      "metadata": {
        "id": "3jbK0PuuXlFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "dd6d51fa-7c2f-4da2-f1eb-eef81f063d70"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard MHSA model ($m_2$)"
      ],
      "metadata": {
        "id": "Z3tA4hXcRwt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model that uses standard MHSA.\n",
        "m_2 = CMT(\n",
        "  in_channels = 3,\n",
        "  stem_channels = 16,\n",
        "  cmt_channels = [46, 92, 184, 368],\n",
        "  patch_channels = [46, 92, 184, 368],\n",
        "  block_layers = [2, 2, 10, 2],\n",
        "  R = 3.6,\n",
        "  img_size = 224,\n",
        "  num_class = 10,\n",
        "  attention_type = \"standard\")\n",
        "\n",
        "# Printing number of parameters.\n",
        "print(f\"m_2 has {sum(p.numel() for p in m_2.parameters())} parameters.\")"
      ],
      "metadata": {
        "id": "us44rHYmDawu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model to device.\n",
        "m_2.to(device)"
      ],
      "metadata": {
        "id": "jcLwrTCwDkT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs.\n",
        "EPOCHS = 25\n",
        "\n",
        "# Initial learning rate.\n",
        "LR = 6e-5\n",
        "\n",
        "# Weight decay.\n",
        "WD = 1e-5\n",
        "\n",
        "# Loss function.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer.\n",
        "optimizer = torch.optim.AdamW(m_2.parameters(), lr = LR, weight_decay = WD)\n",
        "\n",
        "# Scheduler.\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS)"
      ],
      "metadata": {
        "id": "BbK1Lme4DoTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training.\n",
        "history = train(m_2, optimizer, scheduler, loss_fn, train_loader, val_loader, EPOCHS, device)\n",
        "\n",
        "# Creating a checkpoint.\n",
        "checkpoint = {\n",
        "  \"history\": history,\n",
        "  \"model\": m_2.state_dict(),\n",
        "  \"optimizer\": optimizer.state_dict(),\n",
        "  \"scheduler\": scheduler.state_dict()\n",
        "}\n",
        "\n",
        "# Saving the model.\n",
        "torch.save(checkpoint, \"checkpoint2.pt\")"
      ],
      "metadata": {
        "id": "PqQui1oRDr8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting losses and accuracies.\n",
        "plot_training(history)"
      ],
      "metadata": {
        "id": "v2aPYVotD2_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No attention model ($m_3$)"
      ],
      "metadata": {
        "id": "GBo9AEepRzjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model that does not use attention.\n",
        "m_3 = CMT(\n",
        "  in_channels = 3,\n",
        "  stem_channels = 16,\n",
        "  cmt_channels = [46, 92, 184, 368],\n",
        "  patch_channels = [46, 92, 184, 368],\n",
        "  block_layers = [2, 2, 10, 2],\n",
        "  R = 3.6,\n",
        "  img_size = 224,\n",
        "  num_class = 10,\n",
        "  attention_type = None)\n",
        "\n",
        "# Printing number of parameters.\n",
        "print(f\"m_3 has {sum(p.numel() for p in m_3.parameters())} parameters.\")"
      ],
      "metadata": {
        "id": "wX__-lC4FNRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model to device.\n",
        "m_3.to(device)"
      ],
      "metadata": {
        "id": "zCWwDnRHFbOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs.\n",
        "EPOCHS = 25\n",
        "\n",
        "# Initial learning rate.\n",
        "LR = 6e-5\n",
        "\n",
        "# Weight decay.\n",
        "WD = 1e-5\n",
        "\n",
        "# Loss function.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer.\n",
        "optimizer = torch.optim.AdamW(m_3.parameters(), lr = LR, weight_decay = WD)\n",
        "\n",
        "# Scheduler.\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS)"
      ],
      "metadata": {
        "id": "4Y69ErWnFdzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training.\n",
        "history = train(m_3, optimizer, scheduler, loss_fn, train_loader, val_loader, EPOCHS, device)\n",
        "\n",
        "# Creating a checkpoint.\n",
        "checkpoint = {\n",
        "  \"history\": history,\n",
        "  \"model\": m_3.state_dict(),\n",
        "  \"optimizer\": optimizer.state_dict(),\n",
        "  \"scheduler\": scheduler.state_dict()\n",
        "}\n",
        "\n",
        "# Saving the model.\n",
        "torch.save(checkpoint, \"checkpoint3.pt\")"
      ],
      "metadata": {
        "id": "GH9RWNLSFgHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting losses and accuracies.\n",
        "plot_training(history)"
      ],
      "metadata": {
        "id": "CiMNjy4xFliI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Testing"
      ],
      "metadata": {
        "id": "I6gNoTUjPqvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightweight MHSA model ($m_1$)"
      ],
      "metadata": {
        "id": "ClULEfjlPu56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model.\n",
        "y_pred, y_true = test(m_1, test_loader)"
      ],
      "metadata": {
        "id": "gRr29r0ASG8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded7ef90-8c71-4d89-c12c-e7b9963c8c8d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 313/313 [00:35<00:00,  8.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report.\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "g3cCodaVUdpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e1b549-f4a9-4d92-eda7-93c482746c93"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90      1000\n",
            "           1       0.96      0.94      0.95      1000\n",
            "           2       0.86      0.84      0.85      1000\n",
            "           3       0.76      0.77      0.76      1000\n",
            "           4       0.86      0.89      0.88      1000\n",
            "           5       0.82      0.82      0.82      1000\n",
            "           6       0.92      0.92      0.92      1000\n",
            "           7       0.95      0.91      0.93      1000\n",
            "           8       0.93      0.94      0.94      1000\n",
            "           9       0.93      0.93      0.93      1000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard MHSA model ($m_2$)"
      ],
      "metadata": {
        "id": "Fkjn8hY6P0tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model.\n",
        "y_pred, y_true = test(m_2, test_loader)"
      ],
      "metadata": {
        "id": "PbS-9WWnD59a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report.\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "kmsQqiWhD_eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No attention model ($m_3$) "
      ],
      "metadata": {
        "id": "Xen_6qHLP4NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model.\n",
        "y_pred, y_true = test(m_3, test_loader)"
      ],
      "metadata": {
        "id": "pz98i9pyFnk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report.\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "UHQV3I6gFp2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References <a name=\"references\"></a>"
      ],
      "metadata": {
        "id": "PHqUD8h6Gzb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu and Yunhe Wang. CMT: Convolutional Neural Networks Meet Vision Transformers, 2021. [https://arxiv.org/abs/2107.06263](https://arxiv.org/abs/2107.06263)."
      ],
      "metadata": {
        "id": "mVKK69jfHAuM"
      }
    }
  ]
}